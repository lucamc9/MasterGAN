{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lemac/miniconda3/envs/ml/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from model import DCGAN\n",
    "from ops import *\n",
    "from utils import *\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#Load the MNIST dataset\n",
    "mnist = input_data.read_data_sets('MNIST_data/')\n",
    "def conv_out_size_same(size, stride):\n",
    "    return int(math.ceil(float(size) / float(stride)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = mnist.train.next_batch(64)[0].reshape([64, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = mnist.train.next_batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.ConfigProto()\n",
    "with tf.Session(config=run_config) as sess:\n",
    "    dcgan = DCGAN(\n",
    "      sess,\n",
    "      input_width=28,\n",
    "      input_height=28,\n",
    "      output_width=28,\n",
    "      output_height=28,\n",
    "      batch_size=64,\n",
    "      sample_num=64,\n",
    "      y_dim=10,\n",
    "      z_dim=100,\n",
    "      dataset_name='mnist',\n",
    "      input_fname_pattern='*.jpg',\n",
    "      crop=True,\n",
    "      checkpoint_dir=None,\n",
    "      sample_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dcgan.inputs\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dcgan.y\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = dcgan.D\n",
    "discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_logits = dcgan.D_logits\n",
    "d_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' self.D, self.D_logits   = self.discriminator(inputs, self.y, reuse=False) '''\n",
    "\n",
    "def get_d():\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "        # Inits\n",
    "        batch_size = 64\n",
    "        y_dim = 10\n",
    "        c_dim = 1\n",
    "        output_height = 28\n",
    "        output_width = 28\n",
    "        df_dim=64\n",
    "        dfc_dim=1024\n",
    "\n",
    "        # Calcs\n",
    "        y = tf.placeholder(tf.float32, [batch_size, y_dim], name='y')                         # (64, 10)\n",
    "        image_dims = [output_height, output_width, c_dim]                                     # (28, 28, 1)\n",
    "        image = tf.placeholder(tf.float32, [batch_size] + image_dims, name='real_images')     # (64, 28, 28, 1)\n",
    "        d_bn1 = batch_norm(name='d_bn1')\n",
    "        d_bn2 = batch_norm(name='d_bn2')\n",
    "\n",
    "        # Network architecture\n",
    "        yb = tf.reshape(y, [batch_size, 1, 1, y_dim])                                         # (64, 1, 1, 10)\n",
    "        x = conv_cond_concat(image, yb)                                                       # (64, 28, 28, 11)\n",
    "\n",
    "        h0 = lrelu(conv2d(x, c_dim + y_dim, name='d_h0_conv'))                                # (64, 14, 14, 11)\n",
    "        h0 = conv_cond_concat(h0, yb)                                                         # (64, 14, 14, 21)\n",
    "\n",
    "        h1 = lrelu(d_bn1(conv2d(h0, df_dim + y_dim, name='d_h1_conv')))                       # (64, 7, 7, 74)\n",
    "        h1 = tf.reshape(h1, [batch_size, -1])                                                 # (64, 3626)\n",
    "        h1 = concat([h1, y], 1)                                                               # (64, 3636)\n",
    "\n",
    "        h2 = lrelu(d_bn2(linear(h1, dfc_dim, 'd_h2_lin')))                                    # (64, 1024)\n",
    "        h2 = concat([h2, y], 1)                                                               # (64, 1034) \n",
    "\n",
    "        h3 = linear(h2, 1, 'd_h3_lin')                                                        # (64, 1) \n",
    "\n",
    "        output = tf.nn.sigmoid(h3)                                                            # (64, 1)\n",
    "\n",
    "        return output, h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(output_height, output_width, batch_size, y_dim, c_dim, y, z):\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        gfc_dim = 1024\n",
    "        gf_dim = 64\n",
    "        g_bn0 = batch_norm(name='g_bn0')\n",
    "        g_bn1 = batch_norm(name='g_bn1')\n",
    "        g_bn2 = batch_norm(name='g_bn2')\n",
    "\n",
    "        s_h, s_w = output_height, output_width\n",
    "        s_h2, s_h4 = int(s_h/2), int(s_h/4)\n",
    "        s_w2, s_w4 = int(s_w/2), int(s_w/4)\n",
    "\n",
    "        # yb = tf.expand_dims(tf.expand_dims(y, 1),2)\n",
    "        yb = tf.reshape(y, [batch_size, 1, 1, y_dim])\n",
    "        z = concat([z, y], 1)\n",
    "\n",
    "        h0 = tf.nn.relu(g_bn0(linear(z, gfc_dim, 'g_h0_lin')))\n",
    "        h0 = concat([h0, y], 1)\n",
    "\n",
    "        h1 = tf.nn.relu(g_bn1(\n",
    "            linear(h0, gf_dim*2*s_h4*s_w4, 'g_h1_lin')))\n",
    "        h1 = tf.reshape(h1, [batch_size, s_h4, s_w4, gf_dim * 2])\n",
    "\n",
    "        h1 = conv_cond_concat(h1, yb)\n",
    "\n",
    "        h2 = tf.nn.relu(g_bn2(deconv2d(h1,\n",
    "            [batch_size, s_h2, s_w2, gf_dim * 2], name='g_h2')))\n",
    "        h2 = conv_cond_concat(h2, yb)\n",
    "\n",
    "        return tf.nn.sigmoid(\n",
    "            deconv2d(h2, [batch_size, s_h, s_w, c_dim], name='g_h3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inits\n",
    "batch_size = 64\n",
    "y_dim = 10\n",
    "c_dim = 1\n",
    "output_height = 28\n",
    "output_width = 28\n",
    "df_dim=64\n",
    "dfc_dim=1024\n",
    "z_dim = 100\n",
    "z = tf.placeholder(tf.float32, [None, z_dim], name='z')\n",
    "y = tf.placeholder(tf.float32, [batch_size, y_dim], name='y')                         # (64, 10)\n",
    "d_bn1 = batch_norm(name='d_bn1')\n",
    "d_bn2 = batch_norm(name='d_bn2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"generator\") as scope:\n",
    "gfc_dim = 1024\n",
    "gf_dim = 64\n",
    "g_bn0 = batch_norm(name='g_bn0')\n",
    "g_bn1 = batch_norm(name='g_bn1')\n",
    "g_bn2 = batch_norm(name='g_bn2')\n",
    "\n",
    "s_h, s_w = output_height, output_width\n",
    "s_h2, s_h4 = int(s_h/2), int(s_h/4)\n",
    "s_w2, s_w4 = int(s_w/2), int(s_w/4)\n",
    "\n",
    "# yb = tf.expand_dims(tf.expand_dims(y, 1),2)\n",
    "yb = tf.reshape(y, [batch_size, 1, 1, y_dim])\n",
    "z = concat([z, y], 1)\n",
    "\n",
    "h0 = tf.nn.relu(g_bn0(linear(z, gfc_dim, 'g_h0_lin')))\n",
    "h0 = concat([h0, y], 1)\n",
    "\n",
    "h1 = tf.nn.relu(g_bn1(\n",
    "    linear(h0, gf_dim*2*s_h4*s_w4, 'g_h1_lin')))\n",
    "h1 = tf.reshape(h1, [batch_size, s_h4, s_w4, gf_dim * 2])\n",
    "\n",
    "h1 = conv_cond_concat(h1, yb)\n",
    "\n",
    "h2 = tf.nn.relu(g_bn2(deconv2d(h1,\n",
    "    [batch_size, s_h2, s_w2, gf_dim * 2], name='g_h2')))\n",
    "h2 = conv_cond_concat(h2, yb)\n",
    "\n",
    "output = tf.nn.sigmoid(\n",
    "    deconv2d(h2, [batch_size, s_h, s_w, c_dim], name='g_h3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' self.D_, self.D_logits_ = self.discriminator(self.G, self.y, reuse=True) '''\n",
    "\n",
    "def get_d_g():\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "        \n",
    "        scope.reuse_variables()\n",
    "        \n",
    "        # Inits\n",
    "        batch_size = 64\n",
    "        y_dim = 10\n",
    "        c_dim = 1\n",
    "        output_height = 28\n",
    "        output_width = 28\n",
    "        df_dim=64\n",
    "        dfc_dim=1024\n",
    "        z_dim = 100\n",
    "        z = tf.placeholder(tf.float32, [None, z_dim], name='z')\n",
    "\n",
    "\n",
    "        # Calcs\n",
    "        y = tf.placeholder(tf.float32, [batch_size, y_dim], name='y')                         # (64, 10)\n",
    "        d_bn1 = batch_norm(name='d_bn1')\n",
    "        d_bn2 = batch_norm(name='d_bn2')\n",
    "\n",
    "        G = get_generator(output_height, output_width, batch_size, y_dim, c_dim, y, z)        # (64, 28, 28, 1)\n",
    "\n",
    "        # Network architecture\n",
    "        yb = tf.reshape(y, [batch_size, 1, 1, y_dim])                                         # (64, 1, 1, 10)\n",
    "        x = conv_cond_concat(G, yb)                                                           # (64, 28, 28, 11)\n",
    "        h0 = lrelu(conv2d(x, c_dim + y_dim, name='d_h0_conv'))                                # (64, 14, 14, 11)\n",
    "        h0 = conv_cond_concat(h0, yb)                                                         # (64, 14, 14, 21)\n",
    "\n",
    "        h1 = lrelu(d_bn1(conv2d(h0, df_dim + y_dim, name='d_h1_conv')))                       # (64, 7, 7, 74)\n",
    "        h1 = tf.reshape(h1, [batch_size, -1])                                                 # (64, 3626)\n",
    "        h1 = concat([h1, y], 1)                                                               # (64, 3636)\n",
    "\n",
    "        h2 = lrelu(d_bn2(linear(h1, dfc_dim, 'd_h2_lin')))                                    # (64, 1024)\n",
    "        h2 = concat([h2, y], 1)                                                               # (64, 1034) \n",
    "\n",
    "        h3 = linear(h2, 1, 'd_h3_lin')                                                        # (64, 1) \n",
    "\n",
    "        output = tf.nn.sigmoid(h3)                                                            # (64, 1)\n",
    "\n",
    "        return output, h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, D_logits = get_d()\n",
    "D_, D_logits_ = get_d_g()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_cross_entropy_with_logits(x, y):\n",
    "    try:\n",
    "        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)\n",
    "    except:\n",
    "        return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y)\n",
    "\n",
    "\n",
    "d_loss_real = tf.reduce_mean(\n",
    "  sigmoid_cross_entropy_with_logits(D_logits, tf.ones_like(D)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "  sigmoid_cross_entropy_with_logits(D_logits_, tf.zeros_like(D_)))\n",
    "g_loss = tf.reduce_mean(\n",
    "  sigmoid_cross_entropy_with_logits(D_logits_, tf.ones_like(D_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --Training starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n",
    "          .minimize(self.d_loss, var_list=self.d_vars)\n",
    "g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n",
    "          .minimize(self.g_loss, var_list=self.g_vars)\n",
    "try:\n",
    "    tf.global_variables_initializer().run()\n",
    "except:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "self.g_sum = merge_summary([self.z_sum, self.d__sum,\n",
    "  self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])\n",
    "self.d_sum = merge_summary(\n",
    "    [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])\n",
    "self.writer = SummaryWriter(\"./logs\", self.sess.graph)\n",
    "\n",
    "sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim))\n",
    "    \n",
    "if config.dataset == 'mnist':\n",
    "    sample_inputs = self.data_X[0:self.sample_num]\n",
    "    sample_labels = self.data_y[0:self.sample_num]\n",
    "else:\n",
    "    sample_files = self.data[0:self.sample_num]\n",
    "    sample = [\n",
    "    get_image(sample_file,\n",
    "            input_height=self.input_height,\n",
    "            input_width=self.input_width,\n",
    "            resize_height=self.output_height,\n",
    "            resize_width=self.output_width,\n",
    "            crop=self.crop,\n",
    "            grayscale=self.grayscale) for sample_file in sample_files]\n",
    "    \n",
    "if (self.grayscale):\n",
    "    sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None]\n",
    "else:\n",
    "    sample_inputs = np.array(sample).astype(np.float32)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "start_time = time.time()\n",
    "could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "if could_load:\n",
    "    counter = checkpoint_counter\n",
    "    print(\" [*] Load SUCCESS\")\n",
    "else:\n",
    "    print(\" [!] Load failed...\")\n",
    "\n",
    "# Iterate over epochs\n",
    "for epoch in xrange(config.epoch):\n",
    "    if config.dataset == 'mnist':\n",
    "        batch_idxs = min(len(self.data_X), config.train_size) // config.batch_size # get batch indexes\n",
    "    else:      \n",
    "        self.data = glob(os.path.join(\n",
    "          \"./data\", config.dataset, self.input_fname_pattern))\n",
    "        batch_idxs = min(len(self.data), config.train_size) // config.batch_size\n",
    "\n",
    "    for idx in xrange(0, batch_idxs): # iterate over batches\n",
    "        if config.dataset == 'mnist':\n",
    "            batch_images = self.data_X[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "            batch_labels = self.data_y[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "        else:\n",
    "            batch_files = self.data[idx*config.batch_size:(idx+1)*config.batch_size]\n",
    "            batch = [\n",
    "              get_image(batch_file,\n",
    "                        input_height=self.input_height,\n",
    "                        input_width=self.input_width,\n",
    "                        resize_height=self.output_height,\n",
    "                        resize_width=self.output_width,\n",
    "                        crop=self.crop,\n",
    "                        grayscale=self.grayscale) for batch_file in batch_files]\n",
    "            if self.grayscale:\n",
    "            batch_images = np.array(batch).astype(np.float32)[:, :, :, None]\n",
    "            else:\n",
    "            batch_images = np.array(batch).astype(np.float32)\n",
    "\n",
    "    batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\\n",
    "      .astype(np.float32)\n",
    "\n",
    "    if config.dataset == 'mnist':\n",
    "        # Update D network\n",
    "        _, summary_str = self.sess.run([d_optim, self.d_sum],\n",
    "        feed_dict={ \n",
    "          self.inputs: batch_images,\n",
    "          self.z: batch_z,\n",
    "          self.y:batch_labels,\n",
    "        })\n",
    "        self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "        # Update G network\n",
    "        _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "        feed_dict={\n",
    "          self.z: batch_z, \n",
    "          self.y:batch_labels,\n",
    "        })\n",
    "        self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "        # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n",
    "        _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "        feed_dict={ self.z: batch_z, self.y:batch_labels })\n",
    "        self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "        errD_fake = self.d_loss_fake.eval({\n",
    "          self.z: batch_z, \n",
    "          self.y:batch_labels\n",
    "        })\n",
    "        errD_real = self.d_loss_real.eval({\n",
    "          self.inputs: batch_images,\n",
    "          self.y:batch_labels\n",
    "        })\n",
    "        errG = self.g_loss.eval({\n",
    "          self.z: batch_z,\n",
    "          self.y: batch_labels\n",
    "        })\n",
    "    else:\n",
    "        # Update D network\n",
    "        _, summary_str = self.sess.run([d_optim, self.d_sum],\n",
    "        feed_dict={ self.inputs: batch_images, self.z: batch_z })\n",
    "        self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "        # Update G network\n",
    "        _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "        feed_dict={ self.z: batch_z })\n",
    "        self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "        # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n",
    "        _, summary_str = self.sess.run([g_optim, self.g_sum],\n",
    "        feed_dict={ self.z: batch_z })\n",
    "        self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "        errD_fake = self.d_loss_fake.eval({ self.z: batch_z })\n",
    "        errD_real = self.d_loss_real.eval({ self.inputs: batch_images })\n",
    "        errG = self.g_loss.eval({self.z: batch_z})\n",
    "\n",
    "    counter += 1\n",
    "    print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\\n",
    "      % (epoch, idx, batch_idxs,\n",
    "        time.time() - start_time, errD_fake+errD_real, errG))\n",
    "\n",
    "    if np.mod(counter, 100) == 1:\n",
    "        if config.dataset == 'mnist':\n",
    "            samples, d_loss, g_loss = self.sess.run(\n",
    "              [self.sampler, self.d_loss, self.g_loss],\n",
    "              feed_dict={\n",
    "                  self.z: sample_z,\n",
    "                  self.inputs: sample_inputs,\n",
    "                  self.y:sample_labels,\n",
    "              }\n",
    "            )\n",
    "            save_images(samples, image_manifold_size(samples.shape[0]),\n",
    "                  './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx))\n",
    "            print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) \n",
    "        else:\n",
    "        try:\n",
    "            samples, d_loss, g_loss = self.sess.run(\n",
    "            [self.sampler, self.d_loss, self.g_loss],\n",
    "            feed_dict={\n",
    "                self.z: sample_z,\n",
    "                self.inputs: sample_inputs,\n",
    "            },\n",
    "            )\n",
    "            save_images(samples, image_manifold_size(samples.shape[0]),\n",
    "                './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx))\n",
    "            print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) \n",
    "        except:\n",
    "            print(\"one pic error!...\")\n",
    "\n",
    "    if np.mod(counter, 500) == 2:\n",
    "        self.save(config.checkpoint_dir, counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CapsNet version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d_caps():\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "        # Inits\n",
    "#         batch_size = 64\n",
    "#         y_dim = 10\n",
    "#         c_dim = 1\n",
    "#         output_height = 28\n",
    "#         output_width = 28\n",
    "#         df_dim=64\n",
    "#         dfc_dim=1024\n",
    "        caps1_n_maps = 32\n",
    "        caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "        caps1_n_dims = 8\n",
    "\n",
    "        # Calcs\n",
    "#         y = tf.placeholder(tf.float32, [batch_size, y_dim], name='y')                         # (64, 10)\n",
    "#         image_dims = [output_height, output_width, c_dim]                                     # (28, 28, 1)\n",
    "#         image = tf.placeholder(tf.float32, [batch_size] + image_dims, name='real_images')     # (64, 28, 28, 1)\n",
    "#         d_bn1 = batch_norm(name='d_bn1')\n",
    "#         d_bn2 = batch_norm(name='d_bn2')\n",
    "        conv1_params = {\n",
    "        \"filters\": 256,\n",
    "        \"kernel_size\": 9,\n",
    "        \"strides\": 1,\n",
    "        \"padding\": \"valid\",\n",
    "        \"activation\": tf.nn.relu,\n",
    "        }\n",
    "\n",
    "        conv2_params = {\n",
    "            \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
    "            \"kernel_size\": 9,\n",
    "            \"strides\": 2,\n",
    "            \"padding\": \"valid\",\n",
    "            \"activation\": tf.nn.relu\n",
    "        }\n",
    "        \n",
    "        # Network architecture - Primary Capsules\n",
    "        conv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)                             # (?, 20, 20, 256)\n",
    "        conv2 = tf.layers.conv2d(conv1, name=\"conv2\", **conv2_params)                         # (?, 6, 6, 256)\n",
    "        caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims],                       # (?, 1152, 8)\n",
    "                       name=\"caps1_raw\")\n",
    "        caps1_output = squash(caps1_raw, name=\"caps1_output\")                                 # (?, 1152, 8)\n",
    "        \n",
    "        # Network architecture - Digit Capsules\n",
    "        caps2_n_caps = 10\n",
    "        caps2_n_dims = 16\n",
    "        init_sigma = 0.1\n",
    "\n",
    "        W_init = tf.random_normal(\n",
    "            shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "            stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "        W = tf.Variable(W_init, name=\"W\")\n",
    "        batch_size = tf.shape(X)[0]\n",
    "        W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")                       # (?, 1152, 10, 16, 8)\n",
    "        caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                               name=\"caps1_output_expanded\")                 # (?, 1152, 10, 8, 1)\n",
    "        caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                           name=\"caps1_output_tile\")\n",
    "        caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                                     name=\"caps1_output_tiled\")\n",
    "        caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled, \n",
    "                                    name=\"caps2_predicted\")                                  # (?, 1152, 10, 16, 1)\n",
    "        \n",
    "        # Network architecture - Routing\n",
    "        raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
    "                       dtype=np.float32, name=\"raw_weights\")\n",
    "        routing_weights = tf.nn.softmax(raw_weights, dim=2, name=\"routing_weights\")\n",
    "        weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
    "                                   name=\"weighted_predictions\")\n",
    "        weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum\")\n",
    "        caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
    "                              name=\"caps2_output_round_1\")                                    # (?, 1, 10, 16, 1)\n",
    "        caps2_output_round_1_tiled = tf.tile(\n",
    "        caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
    "            name=\"caps2_output_round_1_tiled\")\n",
    "        agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
    "                      transpose_a=True, name=\"agreement\")\n",
    "        raw_weights_round_2 = tf.add(raw_weights, agreement,\n",
    "                             name=\"raw_weights_round_2\")\n",
    "        routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_2\")\n",
    "        weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
    "                                                   caps2_predicted,\n",
    "                                                   name=\"weighted_predictions_round_2\")\n",
    "        weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
    "                                             axis=1, keep_dims=True,\n",
    "                                             name=\"weighted_sum_round_2\")\n",
    "        caps2_output_round_2 = squash(weighted_sum_round_2,\n",
    "                                      axis=-2,\n",
    "                                      name=\"caps2_output_round_2\")\n",
    "        caps2_output = caps2_output_round_2                                                   # (?, 1, 10, 16, 1) \n",
    "\n",
    "        \n",
    "#         yb = tf.reshape(y, [batch_size, 1, 1, y_dim])                                         # (64, 1, 1, 10)\n",
    "#         x = conv_cond_concat(image, yb)                                                       # (64, 28, 28, 11)\n",
    "\n",
    "#         h0 = lrelu(conv2d(x, c_dim + y_dim, name='d_h0_conv'))                                # (64, 14, 14, 11)\n",
    "#         h0 = conv_cond_concat(h0, yb)                                                         # (64, 14, 14, 21)\n",
    "\n",
    "#         h1 = lrelu(d_bn1(conv2d(h0, df_dim + y_dim, name='d_h1_conv')))                       # (64, 7, 7, 74)\n",
    "#         h1 = tf.reshape(h1, [batch_size, -1])                                                 # (64, 3626)\n",
    "#         h1 = concat([h1, y], 1)                                                               # (64, 3636)\n",
    "\n",
    "#         h2 = lrelu(d_bn2(linear(h1, dfc_dim, 'd_h2_lin')))                                    # (64, 1024)\n",
    "#         h2 = concat([h2, y], 1)                                                               # (64, 1034) \n",
    "\n",
    "#         h3 = linear(h2, 1, 'd_h3_lin')                                                        # (64, 1) \n",
    "\n",
    "#         output = tf.nn.sigmoid(h3)                                                            # (64, 1)\n",
    "\n",
    "        return output, h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data\n",
    "input_height = 28\n",
    "input_width = 28\n",
    "c_dim = 1\n",
    "batch_size = 64\n",
    "z_dim = 100\n",
    "\n",
    "# Caps vers\n",
    "mnist = input_data.read_data_sets('MNIST_data/')\n",
    "x_placeholder = tf.placeholder(\"float\", shape=[50, 28,28,1], name='x_placeholder')\n",
    "\n",
    "# Dcgan vers\n",
    "image_dims = [input_height, input_width, c_dim]\n",
    "inputs = tf.placeholder(\n",
    "    tf.float32, [batch_size] + image_dims, name='real_images')\n",
    "\n",
    "x_image = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image.get_shape()\n",
    "\n",
    "d_w1 = tf.get_variable('d_w1', [9, 9, 1, 256], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "d_b1 = tf.get_variable('d_b1', [256], initializer=tf.constant_initializer(0))\n",
    "\n",
    "d1 = tf.nn.conv2d(input=x_image, filter=d_w1, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "d1 = d1 + d_b1\n",
    "\n",
    "with tf.variable_scope('PrimaryCaps_layer'):\n",
    "    primaryCaps = CapsConv(num_units=8, with_routing=False)\n",
    "    caps1 = primaryCaps(d1, num_outputs=256, kernel_size=9, stride=2)\n",
    "    #assert caps1.get_shape() == [128, 1152, 8, 1]\n",
    "\n",
    "with tf.variable_scope('DigitCaps_layer'):\n",
    "    digitCaps = CapsConv(num_units=16, with_routing=True)\n",
    "    caps2 = digitCaps(caps1, num_outputs=10)\n",
    "\n",
    "\n",
    "d_w3 = tf.get_variable('d_w3', [16*10, 1024], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "d_b3 = tf.get_variable('d_b3', [1024], initializer=tf.constant_initializer(0))\n",
    "d3 = tf.reshape(caps2, [-1, 16*10])\n",
    "d3 = tf.matmul(d3, d_w3)\n",
    "d3 = d3 + d_b3\n",
    "d3 = tf.nn.relu(d3)\n",
    "\n",
    "d_w4 = tf.get_variable('d_w4', [1024, 1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "d_b4 = tf.get_variable('d_b4', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "d4 = tf.matmul(d3, d_w4) + d_b4\n",
    "\n",
    "return tf.nn.sigmoid(d4), d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.truncated_normal([batch_size, z_dim], mean=0, stddev=1, name='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first deconv block\n",
    "g_w1 = tf.get_variable('g_w1', [z_dim,102400 ], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "g_b1 = tf.get_variable('g_b1', [102400], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "g1 = tf.matmul(z, g_w1) + g_b1\n",
    "g1 = tf.reshape(g1, [-1, 20, 20, 256])\n",
    "g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='bn1')\n",
    "g1 = tf.nn.relu(g1)\n",
    "print(g1.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CapsNet Implementation\n",
    "\n",
    "# Primary Capsules layer, return [batch_size, 1152, 8, 1]\n",
    "with tf.variable_scope('PrimaryCaps_layerGen'):\n",
    "    primaryCaps = CapsConv2(num_units=8, with_routing=False)\n",
    "    caps1Gen = primaryCaps(g1, num_outputs=256, kernel_size=9, stride=2)\n",
    "    #assert caps1.get_shape() == [128, 1152, 8, 1]\n",
    "\n",
    "# DigitCaps layer, [batch_size, 10, 16, 1]\n",
    "with tf.variable_scope('DigitCaps_layerGen'):\n",
    "    digitCaps = CapsConv2(num_units=16, with_routing=True)\n",
    "    caps2Gen = digitCaps(caps1Gen, num_outputs=10)\n",
    "\n",
    "\n",
    "# Decoder structure in Fig. 2\n",
    "# 1. Do masking, how:\n",
    "with tf.variable_scope('Masking'):\n",
    "    # a). calc ||v_c||, then do softmax(||v_c||)\n",
    "    # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
    "    v_length = tf.sqrt(tf.reduce_sum(tf.square(caps2Gen),\n",
    "                                          axis=2, keep_dims=True) + 1e-9)\n",
    "    softmax_v = tf.nn.softmax(v_length, dim=1)\n",
    "    assert softmax_v.get_shape() == [50, 10, 1, 1]\n",
    "\n",
    "    # b). pick out the index of max softmax val of the 10 caps\n",
    "    # [batch_size, 10, 1, 1] => [batch_size] (index)\n",
    "    argmax_idx = tf.to_int32(tf.argmax(softmax_v, axis=1))\n",
    "    assert argmax_idx.get_shape() == [50, 1, 1]\n",
    "    argmax_idx = tf.reshape(argmax_idx, shape=(50, ))\n",
    "\n",
    "    # Method 1.\n",
    "    if not True:\n",
    "        # c). indexing\n",
    "        # It's not easy to understand the indexing process with argmax_idx\n",
    "        # as we are 3-dim animal\n",
    "        masked_v = []\n",
    "        for batch_size in range(50):\n",
    "            v = caps2Gen[batch_size][argmax_idx[batch_size], :]\n",
    "            masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
    "\n",
    "        masked_v = tf.concat(masked_v, axis=0)\n",
    "        assert masked_v.get_shape() == [50, 1, 16, 1]\n",
    "    # Method 2. masking with true label, default mode\n",
    "    else:\n",
    "        # masked_v = tf.matmul(tf.squeeze(caps2), tf.reshape(Y, (-1, 10, 1)), transpose_a=True)\n",
    "        masked_v = tf.multiply(tf.squeeze(caps2Gen), tf.reshape(np.random.uniform(0.0,9.0,size=10).astype(np.float32), (-1, 10, 1)))\n",
    "        v_length = tf.sqrt(tf.reduce_sum(tf.square(caps2Gen), axis=2, keep_dims=True) + 1e-9)\n",
    "\n",
    "\n",
    "# 2. Reconstructe the MNIST images with 3 FC layers\n",
    "# [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
    "with tf.variable_scope('Decoder'):\n",
    "    vector_j = tf.reshape(caps2Gen, shape=(50, -1))\n",
    "    fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
    "    assert fc1.get_shape() == [50, 512]\n",
    "    fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
    "    assert fc2.get_shape() == [50, 1024]\n",
    "    decoded = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)\n",
    "\n",
    "decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfc_dim = 1024\n",
    "gf_dim = 64\n",
    "batch_size = 64\n",
    "y_dim = 10\n",
    "c_dim = 1\n",
    "output_height = 28\n",
    "output_width = 28\n",
    "z_dim = 100\n",
    "z = tf.placeholder(tf.float32, [None, z_dim], name='z')\n",
    "g_bn0 = batch_norm(name='g_bn0')\n",
    "g_bn1 = batch_norm(name='g_bn1')\n",
    "g_bn2 = batch_norm(name='g_bn2')\n",
    "g_bn3 = batch_norm(name='g_bn3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_h, s_w = output_height, output_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_h8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_h16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project `z` and reshape\n",
    "z_, h0_w, h0_b = linear(\n",
    "    z, gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'g_h0_lin/add:0' shape=(?, 2048) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'g_h0_lin/bias:0' shape=(2048,) dtype=float32_ref>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'g_h0_lin/Matrix:0' shape=(100, 2048) dtype=float32_ref>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = tf.reshape(\n",
    "    z_, [-1, s_h16, s_w16, gf_dim * 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 2, 2, 512) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = tf.nn.relu(g_bn0(h0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(?, 2, 2, 512) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1, h1_w, h1_b = deconv2d(\n",
    "    h0, [batch_size, s_h8, s_w8, gf_dim*4], name='g_h1', with_w=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'g_h1/Reshape:0' shape=(64, 4, 4, 256) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'g_h1/w:0' shape=(5, 5, 256, 512) dtype=float32_ref>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'g_h1/biases:0' shape=(256,) dtype=float32_ref>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = tf.nn.relu(g_bn1(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(64, 4, 4, 256) dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2, h2_w, h2_b = deconv2d(\n",
    "    h1, [batch_size, s_h4, s_w4, gf_dim*2], name='g_h2', with_w=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = tf.nn.relu(g_bn2(h2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3, h3_w, h3_b = deconv2d(\n",
    "    h2, [batch_size, s_h2, s_w2, gf_dim*1], name='g_h3', with_w=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = tf.nn.relu(g_bn3(h3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4, h4_w, h4_b = deconv2d(\n",
    "    h3, [batch_size, s_h, s_w, c_dim], name='g_h4', with_w=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.tanh(h4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Tanh:0' shape=(64, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
