{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lemac/miniconda3/envs/ml/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from model import DCGAN\n",
    "from ops import *\n",
    "from utils import *\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inits\n",
    "batch_size = 64\n",
    "y_dim = 10\n",
    "c_dim = 1\n",
    "output_height = 28\n",
    "output_width = 28\n",
    "df_dim=64\n",
    "dfc_dim=1024\n",
    "z_dim = 100\n",
    "# z = tf.placeholder(tf.float32, [None, z_dim], name='z')\n",
    "# y = tf.placeholder(tf.float32, [batch_size, y_dim], name='y')                         # (64, 10)\n",
    "# d_bn1 = batch_norm(name='d_bn1')\n",
    "# d_bn2 = batch_norm(name='d_bn2')\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.truncated_normal([batch_size, z_dim], mean=0, stddev=1, name='z')\n",
    "#first deconv block\n",
    "g_w1 = tf.get_variable('g_w1', [z_dim,102400 ], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "g_b1 = tf.get_variable('g_b1', [102400], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "g1 = tf.matmul(z, g_w1) + g_b1\n",
    "g1 = tf.reshape(g1, [-1, 20, 20, 256])\n",
    "g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='bn1')\n",
    "g1 = tf.nn.relu(g1)\n",
    "print(g1.get_shape())\n",
    "#CapsNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary Capsules layer, return [batch_size, 1152, 8, 1]\n",
    "primaryCaps = CapsConv2(num_units=8, with_routing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps1Gen = primaryCaps(sess, g1, batch_size, num_outputs=256, kernel_size=9, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DigitCaps layer, [batch_size, 10, 16, 1]\n",
    "digitCaps = CapsConv2(num_units=16, with_routing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "caps2Gen = digitCaps(sess, caps1Gen, batch_size, num_outputs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder structure in Fig. 2\n",
    "# 1. Do masking, how:\n",
    "    # a). calc ||v_c||, then do softmax(||v_c||)\n",
    "    # [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
    "v_length = tf.sqrt(tf.reduce_sum(tf.square(caps2Gen),\n",
    "                        axis=2, keep_dims=True) + 1e-9)\n",
    "softmax_v = tf.nn.softmax(v_length, dim=1)\n",
    "assert softmax_v.get_shape() == [64, 10, 1, 1]\n",
    "\n",
    "# b). pick out the index of max softmax val of the 10 caps\n",
    "# [batch_size, 10, 1, 1] => [batch_size] (index)\n",
    "argmax_idx = tf.to_int32(tf.argmax(softmax_v, axis=1))\n",
    "assert argmax_idx.get_shape() == [64, 1, 1]\n",
    "argmax_idx = tf.reshape(argmax_idx, shape=(64, ))\n",
    "\n",
    "# Method 1.\n",
    "if not True:\n",
    "    # c). indexing\n",
    "    # It's not easy to understand the indexing process with argmax_idx\n",
    "    # as we are 3-dim animal\n",
    "    masked_v = []\n",
    "    for batch_size in range(64):\n",
    "        v = caps2Gen[batch_size][argmax_idx[batch_size], :]\n",
    "        masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
    "\n",
    "    masked_v = tf.concat(masked_v, axis=0)\n",
    "    assert masked_v.get_shape() == [64, 1, 16, 1]\n",
    "# Method 2. masking with true label, default mode\n",
    "else:\n",
    "    # masked_v = tf.matmul(tf.squeeze(caps2), tf.reshape(Y, (-1, 10, 1)), transpose_a=True)\n",
    "    masked_v = tf.multiply(tf.squeeze(caps2Gen), tf.reshape(np.random.uniform(0.0,9.0,size=10).astype(np.float32), (-1, 10, 1)))\n",
    "    v_length = tf.sqrt(tf.reduce_sum(tf.square(caps2Gen), axis=2, keep_dims=True) + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Reconstructe the MNIST images with 3 FC layers\n",
    "# [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
    "vector_j = tf.reshape(caps2Gen, shape=(64, -1))\n",
    "fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
    "assert fc1.get_shape() == [64, 512]\n",
    "fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
    "assert fc2.get_shape() == [64, 1024]\n",
    "\n",
    "output = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsConv(object):\n",
    "    ''' Capsule layer.\n",
    "    Args:\n",
    "        input: A 4-D tensor.\n",
    "        num_units: integer, the length of the output vector of a capsule.\n",
    "        with_routing: boolean, this capsule is routing with the\n",
    "        lower-level layer capsule.\n",
    "        num_outputs: the number of capsule in this layer.\n",
    "    Returns:\n",
    "        A 4-D tensor.\n",
    "    '''\n",
    "    def __init__(self, num_units, with_routing=True):\n",
    "        self.num_units = num_units\n",
    "        self.with_routing = with_routing\n",
    "    def __call__(self, input, num_outputs, batch_size, kernel_size=None, stride=None):\n",
    "        self.num_outputs = num_outputs\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.batch_size = batch_size\n",
    "        if not self.with_routing:\n",
    "            #assert input.get_shape() == [None, 28,28,256]\n",
    "\n",
    "            capsules = tf.contrib.layers.conv2d(input, self.num_outputs,\n",
    "                            self.kernel_size, self.stride, padding=\"VALID\",\n",
    "                            activation_fn=tf.nn.relu)\n",
    "            capsules = tf.reshape(capsules, (self.batch_size, -1, self.num_units, 1))\n",
    "\n",
    "            # [batch_size, 1152, 8, 1]\n",
    "            capsules = squash(capsules)\n",
    "            print(capsules.get_shape())\n",
    "            assert capsules.get_shape() == [self.batch_size, 1152, 8, 1]\n",
    "            return(capsules)\n",
    "        else:\n",
    "            # the DigitCap layer\n",
    "            # reshape the input into shape [128,1152,8,1]\n",
    "            input = tf.reshape(input, shape=(self.batch_size, 1152, 8,1))\n",
    "\n",
    "            #b_IJ : [1, num_caps_1, num_caps_1_plus_1, 1]\n",
    "            b_IJ = tf.zeros(shape=[1,1152,10,1], dtype=np.float32)\n",
    "            capsules = []\n",
    "            for j in range(self.num_outputs):\n",
    "                with tf.variable_scope('caps_' + str(j)):\n",
    "                    caps_j, b_IJ = capsule(input, b_IJ, j)\n",
    "                    capsules.append(caps_j)\n",
    "\n",
    "            #return a tensor with shape [batch_size,10,16,1]\n",
    "            capsules = tf.concat(capsules, axis=1)\n",
    "            assert capsules.get_shape() == [self.batch_size,10,16,1]\n",
    "\n",
    "        return(capsules)\n",
    "\n",
    "class CapsConv2(object):\n",
    "    ''' Capsule layer.\n",
    "    Args:\n",
    "        input: A 4-D tensor.\n",
    "        num_units: integer, the length of the output vector of a capsule.\n",
    "        with_routing: boolean, this capsule is routing with the\n",
    "        lower-level layer capsule.\n",
    "        num_outputs: the number of capsule in this layer.\n",
    "    Returns:\n",
    "        A 4-D tensor.\n",
    "    '''\n",
    "    def __init__(self, num_units, with_routing=True):\n",
    "        self.num_units = num_units\n",
    "        self.with_routing = with_routing\n",
    "    def __call__(self, input, batch_size, num_outputs, kernel_size=None, stride=None):\n",
    "        self.num_outputs = num_outputs\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.batch_size = batch_size\n",
    "        if not self.with_routing:\n",
    "            #assert input.get_shape() == [None, 28,28,256]\n",
    "\n",
    "            capsules = tf.contrib.layers.conv2d(input, self.num_outputs,\n",
    "                        self.kernel_size, self.stride, padding=\"VALID\",\n",
    "                        activation_fn=tf.nn.relu)\n",
    "            capsules = tf.reshape(capsules, (self.batch_size, -1, self.num_units, 1))\n",
    "\n",
    "            # [batch_size, 1152, 8, 1]\n",
    "            capsules = squash(capsules)\n",
    "            print(capsules.get_shape())\n",
    "            assert capsules.get_shape() == [self.batch_size, 1152, 8, 1]\n",
    "            return(capsules)\n",
    "\n",
    "        else:\n",
    "            # the DigitCap layer\n",
    "            # reshape the input into shape [128,1152,8,1]\n",
    "            input = tf.reshape(input, shape=(self.batch_size, 1152, 8,1))\n",
    "\n",
    "            #b_IJ : [1, num_caps_1, num_caps_1_plus_1, 1]\n",
    "            b_IJ = tf.zeros(shape=[1,1152,10,1], dtype=np.float32)\n",
    "            capsules = []\n",
    "            for j in range(self.num_outputs):\n",
    "                with tf.variable_scope('caps_' + str(j)):\n",
    "                    caps_j, b_IJ = capsule(input, b_IJ, j)\n",
    "                    capsules.append(caps_j)\n",
    "\n",
    "                    #return a tensor with shape [batch_size,10,16,1]\n",
    "                    capsules = tf.concat(capsules, axis=1)\n",
    "                    assert capsules.get_shape() == [self.batch_size,10,16,1]\n",
    "\n",
    "        return(capsules)\n",
    "\n",
    "def capsule(input, b_IJ, idx_j):\n",
    "    ''' The routing algorithm for one capsule in the layer l+1.\n",
    "    Args:\n",
    "        input: A Tensor with [batch_size, num_caps_l=1152, length(u_i)=8, 1]\n",
    "        shape, num_caps_l meaning the number of capsule in the layer l.\n",
    "    Returns:\n",
    "        A Tensor of shape [batch_size, 1, length(v_j)=16, 1] representing the\n",
    "        vector output `v_j` of capsule j in the layer l+1\n",
    "    Notes:\n",
    "        u_i represents the vector output of capsule i in the layer l, and\n",
    "        v_j the vector output of capsule j in the layer l+1.\n",
    "    '''\n",
    "    with tf.variable_scope('routing'):\n",
    "\n",
    "        w_initializer = np.random.normal(size=[1, 1152, 8, 16], scale=0.01)\n",
    "\n",
    "        W_Ij = tf.Variable(w_initializer, dtype=tf.float32)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # repeat W_Ij with batch_size times to shape [batch_size, 1152, 8, 16]\n",
    "        W_Ij = tf.tile(W_Ij, [64, 1, 1, 1])\n",
    "\n",
    "        # calc u_hat\n",
    "        # [8, 16].T x [8, 1] => [16, 1] => [batch_size, 1152, 16, 1]\n",
    "        u_hat = tf.matmul(W_Ij, input, transpose_a=True)\n",
    "        assert u_hat.get_shape() == [64, 1152, 16, 1]\n",
    "\n",
    "        shape = b_IJ.get_shape().as_list()\n",
    "        size_splits = [idx_j, 1, shape[2] - idx_j - 1]\n",
    "        for r_iter in range(3):\n",
    "            # line 4:\n",
    "            # [1, 1152, 10, 1]\n",
    "            c_IJ = tf.nn.softmax(b_IJ, dim=2)\n",
    "            assert c_IJ.get_shape() == [1, 1152, 10, 1]\n",
    "\n",
    "            # line 5:\n",
    "            # weighting u_hat with c_I in the third dim,\n",
    "            # then sum in the second dim, resulting in [batch_size, 1, 16, 1]\n",
    "            b_Il, b_Ij, b_Ir = tf.split(b_IJ, size_splits, axis=2)\n",
    "            c_Il, c_Ij, b_Ir = tf.split(c_IJ, size_splits, axis=2)\n",
    "            assert c_Ij.get_shape() == [1, 1152, 1, 1]\n",
    "\n",
    "            s_j = tf.multiply(c_Ij, u_hat)\n",
    "            s_j = tf.reduce_sum(tf.multiply(c_Ij, u_hat),\n",
    "                                axis=1, keep_dims=True)\n",
    "            assert s_j.get_shape() == [64, 1, 16, 1]\n",
    "\n",
    "            # line 6:\n",
    "            # squash using Eq.1, resulting in [batch_size, 1, 16, 1]\n",
    "            v_j = squash(s_j)\n",
    "            assert s_j.get_shape() == [64, 1, 16, 1]\n",
    "\n",
    "            # line 7:\n",
    "            # tile v_j from [batch_size ,1, 16, 1] to [batch_size, 1152, 16, 1]\n",
    "            # [16, 1].T x [16, 1] => [1, 1], then reduce mean in the\n",
    "            # batch_size dim, resulting in [1, 1152, 1, 1]\n",
    "            v_j_tiled = tf.tile(v_j, [1, 1152, 1, 1])\n",
    "            u_produce_v = tf.matmul(u_hat, v_j_tiled, transpose_a=True)\n",
    "            assert u_produce_v.get_shape() == [64, 1152, 1, 1]\n",
    "            b_Ij += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
    "            b_IJ = tf.concat([b_Il, b_Ij, b_Ir], axis=2)\n",
    "\n",
    "        return(v_j, b_IJ)\n",
    "\n",
    "def squash(vector):\n",
    "    '''Squashing function.\n",
    "    Args:\n",
    "        vector: A 4-D tensor with shape [batch_size, num_caps, vec_len, 1],\n",
    "    Returns:\n",
    "        A 4-D tensor with the same shape as vector but\n",
    "        squashed in 3rd and 4th dimensions.\n",
    "    '''\n",
    "    vec_abs = tf.sqrt(tf.reduce_sum(tf.square(vector)))  # a scalar\n",
    "    scalar_factor = tf.square(vec_abs) / (1 + tf.square(vec_abs))\n",
    "    vec_squashed = scalar_factor * tf.divide(vector, vec_abs)  # element-wise\n",
    "    return(vec_squashed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP DEBUGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing CapsConv2 standalone --done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inits\n",
    "batch_size = 64\n",
    "y_dim = 10\n",
    "c_dim = 1\n",
    "output_height = 28\n",
    "output_width = 28\n",
    "df_dim=64\n",
    "dfc_dim=1024\n",
    "z_dim = 100\n",
    "\n",
    "z = tf.truncated_normal([batch_size, z_dim], mean=0, stddev=1, name='z')\n",
    "#first deconv block\n",
    "g_w1 = tf.get_variable('g_w1', [z_dim,102400 ], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "g_b1 = tf.get_variable('g_b1', [102400], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "g1 = tf.matmul(z, g_w1) + g_b1\n",
    "g1 = tf.reshape(g1, [-1, 20, 20, 256])\n",
    "g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='bn1')\n",
    "g1 = tf.nn.relu(g1)\n",
    "print(g1.get_shape())\n",
    "#CapsNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 8\n",
    "x = g1 \n",
    "num_outputs = 256\n",
    "kernel_size = 9\n",
    "stride = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsules = tf.contrib.layers.conv2d(g1, num_outputs,\n",
    "                                    kernel_size, stride, padding=\"VALID\",\n",
    "                                    activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsules = tf.reshape(capsules, (64, -1, num_units, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, 1152, 8, 1]\n",
    "capsules = squash(capsules)\n",
    "print(capsules.get_shape())\n",
    "assert capsules.get_shape() == [64, 1152, 8, 1]\n",
    "output = capsules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "z_dim = 100\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.truncated_normal([batch_size, z_dim], mean=0, stddev=1, name='z')\n",
    "#first deconv block\n",
    "g_w1 = tf.get_variable('g_w1', [z_dim,102400 ], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "g_b1 = tf.get_variable('g_b1', [102400], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "g1 = tf.matmul(z, g_w1) + g_b1\n",
    "g1 = tf.reshape(g1, [-1, 20, 20, 256])\n",
    "g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='bn1')\n",
    "g1 = tf.nn.relu(g1)\n",
    "print(g1.get_shape())\n",
    "#CapsNet Implementation\n",
    "\n",
    "# Primary Capsules layer, return [batch_size, 1152, 8, 1]\n",
    "primaryCaps = CapsConv2(num_units=8, with_routing=False)\n",
    "caps1Gen = primaryCaps(sess, g1, num_outputs=256, batch_size=batch_size, kernel_size=9, stride=2)\n",
    "#assert caps1.get_shape() == [128, 1152, 8, 1]\n",
    "\n",
    "# DigitCaps layer, [batch_size, 10, 16, 1]\n",
    "digitCaps = CapsConv2(num_units=16, with_routing=True)\n",
    "caps2Gen = digitCaps(sess, caps1Gen, num_outputs=10, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Decoder structure in Fig. 2\n",
    "# 1. Do masking, how:\n",
    "# a). calc ||v_c||, then do softmax(||v_c||)\n",
    "# [batch_size, 10, 16, 1] => [batch_size, 10, 1, 1]\n",
    "v_length = tf.sqrt(tf.reduce_sum(tf.square(caps2Gen),\n",
    "                                      axis=2, keep_dims=True) + 1e-9)\n",
    "softmax_v = tf.nn.softmax(v_length, dim=1)\n",
    "assert softmax_v.get_shape() == [64, 10, 1, 1]\n",
    "\n",
    "# b). pick out the index of max softmax val of the 10 caps\n",
    "# [batch_size, 10, 1, 1] => [batch_size] (index)\n",
    "argmax_idx = tf.to_int32(tf.argmax(softmax_v, axis=1))\n",
    "assert argmax_idx.get_shape() == [64, 1, 1]\n",
    "argmax_idx = tf.reshape(argmax_idx, shape=(64, ))\n",
    "\n",
    "# Method 1.\n",
    "if not True:\n",
    "    # c). indexing\n",
    "    # It's not easy to understand the indexing process with argmax_idx\n",
    "    # as we are 3-dim animal\n",
    "    masked_v = []\n",
    "    for batch_size in range(64):\n",
    "        v = caps2Gen[batch_size][argmax_idx[batch_size], :]\n",
    "        masked_v.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
    "\n",
    "    masked_v = tf.concat(masked_v, axis=0)\n",
    "    assert masked_v.get_shape() == [64, 1, 16, 1]\n",
    "# Method 2. masking with true label, default mode\n",
    "else:\n",
    "    # masked_v = tf.matmul(tf.squeeze(caps2), tf.reshape(Y, (-1, 10, 1)), transpose_a=True)\n",
    "    masked_v = tf.multiply(tf.squeeze(caps2Gen), tf.reshape(np.random.uniform(0.0,9.0,size=10).astype(np.float32), (-1, 10, 1)))\n",
    "    v_length = tf.sqrt(tf.reduce_sum(tf.square(caps2Gen), axis=2, keep_dims=True) + 1e-9)\n",
    "\n",
    "\n",
    "# 2. Reconstructe the MNIST images with 3 FC layers\n",
    "# [batch_size, 1, 16, 1] => [batch_size, 16] => [batch_size, 512]\n",
    "vector_j = tf.reshape(caps2Gen, shape=(64, -1))\n",
    "fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
    "assert fc1.get_shape() == [64, 512]\n",
    "fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
    "assert fc2.get_shape() == [64, 1024]\n",
    "decoded = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fully_connected_2/Sigmoid:0' shape=(64, 784) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_10:0' shape=(64, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(decoded, shape=(64, 28, 28, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
